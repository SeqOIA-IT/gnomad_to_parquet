fonctionne : 
./bin/spark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:0.14.0 \
    --driver-memory 50g --executor-memory 20g \
    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
    --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
    --conf spark.sql.catalog.spark_catalog.type=hive \
    --conf spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog \
    --conf spark.sql.catalog.my_catalog.type=hadoop \
    --conf spark.sql.catalog.my_catalog.warehouse=/srv/lfrobert/spark-3.3.3-bin-hadoop3/test_my_catalog2

create database my_catalog.toto;
drop table my_catalog.toto.my_table2
SET spark.sql.parser.quotedRegexColumnNames=true


create table my_catalog.toto.my_table2 USING iceberg 
PARTITIONED BY (chrom)
TBLPROPERTIES ('sort_order' = 'pos ASC NULLS LAST','write.distribution-mode'='range','write.target-file-size-bytes'='134217728','write.parquet.compression-codec'='snappy') 
as
select 24 as chrom, `^(?!.*(chrom)).*$` from parquet.`/srv/lfrobert/gnomad_to_parquet/gnomad_v4_chromY.parquet`;
 
 
TODO : optimize pour merge 
et delete orphans

Time taken: 18.103 seconds

